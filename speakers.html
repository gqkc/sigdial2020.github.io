<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  
    <title>Keynote Speakers</title>

    <!-- Bootstrap -->
    <link rel="stylesheet" type="text/css" href="assets/css/bootstrap.min.css" >    
    <!-- Main Style -->
    <link rel="stylesheet" type="text/css" href="assets/css/main.css">
    <!-- Responsive Style -->
    <link rel="stylesheet" type="text/css" href="assets/css/responsive.css">
    <!-- Fonts -->
    <link rel="stylesheet" type="text/css" href="assets/fonts/font-awesome.min.css">
    <!-- Icon -->
    <link rel="stylesheet" type="text/css" href="assets/fonts/simple-line-icons.css"> 
    <!-- Slicknav -->
    <link rel="stylesheet" type="text/css" href="assets/css/slicknav.css">
    <!-- Nivo Lightbox -->
    <link rel="stylesheet" type="text/css" href="assets/css/nivo-lightbox.css" > 
    <!-- Animate -->
    <link rel="stylesheet" type="text/css" href="assets/css/animate.css">
    <!-- Owl carousel -->
    <link rel="stylesheet" type="text/css" href="assets/css/owl.carousel.css">   
    

    <!-- HTML5 shim and Respond.js.txt for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js.txt doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js.txt"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js.txt"></script>
    <![endif]-->
      <script src="assets/js/jquery.min.js.txt"></script>
<script> $(function(){ $("#includedContent").load("headermenu.html"); }); </script> 
<!-- <script> $(function(){ $("#speakersContent").load("content/speakerbios.html"); }); </script> -->
<script> $(function(){ $("#footerContent").load("footer.html"); }); </script> 
  </head>
  <body>

 <div id="includedContent"></div> 

    <!-- Page Header Start -->
    <div class="page-header">      
      <div class="container"> 
        <div class="page-header-inner">         
          <ol class="breadcrumb wow fadeInDown" data-wow-delay="300ms">
            <li><a href="index.html">Home</a></li>
            <li class="page">Speakers</li>
          </ol>
          <h1 class="page-title wow fadeInRight" data-wow-delay="300ms">
            Speakers
          </h1>
        </div>
      </div>
    </div>
    <!-- Page Header End-->

    <!-- Team Area Section Start -->
    <section id="team" class="section gray-bg">
      <div class="container">
        <div class="row">
          <div class="col-md-12">
            <h2 class="section-title wow fadeInUp" data-wow-delay="0s">Keynote Speakers</h2>

            <h3>Keynote Speakers: </h3>
            <p>
             <b id="speaker_celikyilmaz">Asli Celikyilmaz, Microsoft Research</b></br>


<b>Title</b>: Neural Text Generation: Progress and Challenges 
<br>
<b>Abstract</b>: Automatic text generation enables computers to summarize text, describe pictures to visually impaired, write stories or articles about an event, have conversations in customer-service, chit-chat with individuals, and other settings, etc. Neural text generation – using neural network models to generate coherent text – have seen a paradigm shift in the last years, caused by the advances in deep contextual language modeling (e.g., LSTMs, GPT) and transfer learning (e.g., ELMO, BERT). While these tools have dramatically improved the state of text generation, particularly for low resource tasks, state-of-the-art neural text generation models still face many challenges: a lack of diversity in generated text, commonsense violations in depicted situations, difficulties in making use of multi-modal input, and many more. I will discuss existing technology to generate text with better discourse structure, narrative flow, or one that can use world knowledge more intelligently. I will conclude the talk with a discussion of current challenges and shortcomings of neural text generation, pointing to avenues for future research.  
 
<br>
<b>Bio</b>: 
Asli Celikyilmaz is a Principal Researcher at Microsoft Research (MSR) in Redmond, Washington. She is also an Affiliate Professor at the University of Washington. She has received Ph.D. Degree in Information Science from University of Toronto, Canada, and later continued her Postdoc study at Computer Science Department of the University of California, Berkeley. Her research interests are mainly in deep learning and natural language, specifically on language generation with long-term coherence, language understanding, language grounding with vision, and building intelligent agents for human-computer interaction   She is  serving on the editorial boards of Transactions of the ACL (TACL) as area editor and Open Journal of Signal Processing (OJSP) as Associate Editor. She has received several “best of” awards including NAFIPS 2007, Semantic Computing 2009, and CVPR 2019.


<br><br>

             <b id="speaker_litman">Diane Litman, University of Pittsburgh</b></br>


             <b>Title</b>: Argument Mining, Discourse Analysis, and Educational Applications 
<br>
<b>Abstract</b>: The written and spoken arguments of students are educational data that can be automatically mined for purposes such as student assessment or teacher professional development.  This talk will illustrate some of the opportunities and challenges in educationally-oriented argument mining. I will first describe how we are using discourse analysis to improve argument mining systems that are being embedded in educational technologies for essay grading and for analyzing classroom discussions. I will then present intrinsic and extrinsic evaluation results for two of our argument mining systems, using benchmark persuasive essay corpora as well as our recently released Discussion Tracker corpus of collaborative argumentation in high school classrooms.

<br>
<b>Bio</b>: Diane Litman is Professor of Computer Science, Senior Scientist with the Learning Research and Development Center, and Faculty Co-Director of the Graduate Program in Intelligent Systems, all at the University of Pittsburgh.  Her current research focuses on enhancing the effectiveness of educational technology through the use of spoken and natural language processing techniques such as argument mining, summarization, multi-party dialogue systems, and revision analysis.  She is a Fellow of the Association for Computational Linguistics, has twice been elected Chair of the North American Chapter of the Association for Computational Linguistics, has co-authored multiple papers winning best paper awards, and was the SIGdial Program Co-Chair in 2018.


<br><br>
             <b id="speaker_skantze">Gabriel Skantze, KTH Royal Institute of Technologies</b></br>

             <b>Title</b>: Conversational Turn-taking in Human-robot Interaction
             <br>
             <b>Abstract</b>: The last decade has seen a breakthrough for speech interfaces, much thanks to the advancements in speech recognition. Apart from voice assistants in smart speakers and phones, an emerging application area are social robots, which are expected to serve as receptionists, teachers, companions, coworkers, etc. Just like we prefer physical meetings over phone calls and video conferencing, social robots can potentially offer a much richer interaction experience than non-embodied dialogue systems. One example of this is the Furhat robot head, which started as a research project at KTH, but is now used in commercial applications, such as serving as a concierge at airports and performing job interviews.

             However, even though this recent progress is very exciting, current dialogue systems are still limited in several ways, especially for human-robot interaction. In this talk, I will specifically address the modelling of conversational turn-taking. As current systems lack the sophisticated coordination mechanisms found in human-human interaction, they are often plagued by interruptions or sluggish responses. In a face-to-face conversation, we use various multi-modal signals for this coordination, including linguistic and prosodic cues, as well as gaze and gestures. I will present our work on the use of deep learning for modelling these cues, which can allow the system to predict, and even project, potential turn-shifts. I will also present user studies which show how the robot can regulate turn-taking in multi-party dialogue by employing various turn-taking signals. This can be used to both facilitate a smoother interaction, as well as shaping the turn-taking dynamics and participation equality in multi-party settings.
             <br>
             <b>Bio</b>: Gabriel Skantze is professor in speech technology with a specialization in dialogue systems at KTH Royal Institute of Technology. His research focuses on the development of computational models for situated dialogue and human-robot interaction. He is also co-founder and chief scientist at Furhat Robotics, a startup based in Stockholm developing a platform for social robotics. Since 2019, he is the president of SIGdial. 

            </p>
           
<!--
            <div class="col-md-4 col-sm-6 col-xs-12">
              <div class="speakers-member wow fadeIn" data-wow-delay="0.1s">
	        <div class="member-img">
		  <a href="https://www.sigdial.org/files/workshops/conference20/speakers.html#bohus"><img src="assets/img/teacher/bohus.jpg" height="225"></a>
                </div>
                <div class="member-desc">
                  <h3>Dan Bohus</h3>
                  <h5>Microsoft Research, USA</h5>
                </div>              
              </div>
            </div>

            <div class="col-md-4 col-sm-6 col-xs-12">
              <div class="speakers-member wow fadeIn" data-wow-delay="0.3s">

                <div class="member-img">
                  <a href="https://www.sigdial.org/files/workshops/conference20/speakers.html#lapata"><img src="assets/img/teacher/lapata.jpg" height="225"></a>
                </div>
                 <div class="member-desc">
                  <h3>Mirella Lapata</h3>
                  <h5>University of Edinburgh, UK</h5>
                 </div>
               </div>
             </div>

             <div class="col-md-4 col-sm-6 col-xs-12">
               <div class="speakers-member wow fadeIn" data-wow-delay="0.5s">
                 <div class="member-img">
	           <img src="assets/img/teacher/meng.jpg" height="225">
                 </div>
                 <div class="member-desc">
                   <h3>Helen Meng</h3>
                   <h5>Chinese University of Hong Kong, China</h5>
                 </div>              
               </div>
             </div>
          </div> 

          
        
	</div>

     <h4 id="bohus"><a href="https://www.microsoft.com/en-us/research/people/dbohus/">Dan Bohus</a></h4>
	<p>Senior Principal Researcher, Perception and Interaction Group<br>
           Microsoft Research, Redmond, Washington, US</p>
     <b>Title</b>
     <p>Situated Interaction</p>
     <b>Abstract</b>
     <p>Physically situated dialog is a complex, multimodal affair that goes well beyond the spoken word. When interacting with each other, people incrementally coordinate their actions to simultaneously resolve several different problems: they manage engagement, coordinate on taking turns, recognize intentions, and establish and maintain common ground as a basis for contributing to the conversation. A wide array of non-verbal signals are brought to bear. Proximity and body pose, attention and gaze, head nods and hand gestures, prosody and facial expressions, all play important roles in the intricate, mixed-initiative, fluidly coordinated process we call interaction. And just like a couple of decades ago advances in speech recognition opened up the field of spoken dialog systems, today advances in vision and other perceptual technologies are again opening up new horizons -- we are starting to be able to build machines that can understand these social signals and the physical world around them, and begin to participate in physically situated interactions and collaborations with people.
     </p>
     <p>In this talk, using a number of research vignettes from my work, I will draw attention to some of the challenges and opportunities that lie ahead of us in this exciting space. In particular, I will discuss issues with managing engagement and turn-taking in multiparty open-world settings, and more generally highlight the importance of timing and fine-grained coordination in situated interaction. Finally, I will conclude by describing a framework that promises to simplify the development of physically situated interactive systems and enable more research and faster progress in this area.
     </p>
     <b>Biography</b>
     <p>Dan Bohus is a Senior Principal Researcher in the Perception and Interaction Group at Microsoft Research. His work centers on the study and development of computational models for physically situated spoken language interaction and collaboration. The long term question that shapes his research agenda is how can we enable interactive systems to reason more deeply about their surroundings and seamlessly participate in open-world, multiparty dialog and collaboration with people? Prior to joining Microsoft Research, Dan obtained his Ph.D. from Carnegie Mellon University.
     </p>
     <br>

     <h4 id="lapata"><a href="https://homepages.inf.ed.ac.uk/mlap/">Mirella Lapata</a></h4>
     <p>Professor, School of Informatics<br> 
        University of Edinburgh, UK</p>
     <b>Title</b>
     <p>Learning Natural Language Interfaces with Neural Models</p>
     <b>Abstract</b>
     <p>In Spike Jonze's futuristic film "Her", Theodore, a lonely writer, forms a strong emotional bond with Samantha, an operating system designed to meet his every need. Samantha can carry on seamless conversations with Theodore, exhibits a perfect command of language, and is able to take on complex tasks. She filters his emails for importance, allowing him to deal with information overload, she proactively arranges the publication of Theodore's letters, and is able to give advice using common sense and reasoning skills.
     </p>
     <p>In this talk I will present an overview of recent progress on learning natural language interfaces which might not be as clever as Samantha but nevertheless allow uses to interact with various devices and services using every day language. I will address the structured prediction problem of mapping natural language utterances onto machine-interpretable representations and outline the various challenges it poses. For example, the fact that the translation of natural language to formal language is highly non-isomorphic, data for model training is scarce, and natural language can express the same information need in many different ways. I will describe a general modeling framework based on neural networks which tackles these challenges and improves the robustness of natural language interfaces.
     </p>
     <b>Biography</b>
     <p>Mirella Lapata is professor of natural language processing in the School of Informatics at the University of Edinburgh. Her research focuses on getting computers to understand, reason with, and generate natural language. She is the first recipient (2009) of the British Computer Society and Information Retrieval Specialist Group (BCS/IRSG) Karen Sparck Jones award and a Fellow of the Royal Society of Edinburgh. She has also received best paper awards in leading NLP conferences and has served on the editorial boards of the Journal of Artificial Intelligence Research, the Transactions of the ACL, and Computational Linguistics. She was president of SIGDAT (the group that organizes EMNLP) in 2018.
     </p>
<br>

     <h4 id="meng"><a href="http://www.se.cuhk.edu.hk/people/hmmeng.html">Helen Meng</a></h4>
	<p>Professor, Department of Systems Engineering and Engineering Management<br>
           Chinese University of Hong Kong, China
	</p>
     <b>Title</b>
     <p>The Many Facets of Dialog</p>
     <b>Abstract</b>
     <p>Dialog is a most fascinating form of human communication.  The back-and-forth exchanges convey the speaker's message to the listener, and the listener can derive information about the speaker's thoughts, intent, well-being, emotions and much more.  This talk presents an overview of dialog research that concerns our group at The Chinese University of Hong Kong.  In the domain of education and learning, we have been recording in-class student group discussions in the flipped-classroom setting of a freshman elite mathematics course.  We investigate features in the weekly, within-group dialogs that may relate to class performance and learning efficacy.  In the domain of e-commerce, we are developing dialog models based on approximately 20 million conversation turns, to support a virtual shopping assistant in customer inquiries and orders, logistics tracking, etc.  In the domain of health and wellbeing, we are capturing and analysing dialogs between health professionals (or their virtual equivalent) and subjects in cognitive screening tests.  We also conduct research in both semantic interpretation and dialog state tracking, as well as affective design of virtual conversational assistants.  For the former, we have developed a Convex Polytopic Model for extracting a knowledge representation from user inputs in dialog turns by generating a compact convex polytope to enclose all the data points projected to a latent semantic space.  The polytope vertices represent extracted semantic concepts.  Each user input can then be "interpreted" as a sequence of polytope vertices which represent the user's goals and dialog states.  For the latter, we have developed a multimodal, multi-task, deep learning framework to infer the user's emotive state and emotive state change simultaneously.  This enables virtual conversational assistants to understand the emotive state in the user's input and to generate an appropriate emotive system response in the dialog turn, which will further influence the user's emotive state in the subsequent dialog turn.  Such an affective design will be able to enhance user experience in conversational dialogs with intelligent virtual assistants.
     </p>
     <b>Biography</b>
     <p>Helen Meng is Patrick Huen Wing Ming Professor of Systems Engineering and Engineering Management at The Chinese University of Hong Kong (CUHK). She is the Founding Director of the CUHK Ministry of Education (MoE)-Microsoft Key Laboratory for Human-Centric Computing and Interface Technologies (since 2005), Tsinghua-CUHK Joint Research Center for Media Sciences, Technologies and Systems (since 2006), and Co-Director of the Stanley Ho Big Data Decision Analytics Research Center (since 2013). Previously, she served as CUHK Faculty of Engineering's Associate Dean (Research), Chairman of the Department of Systems Engineering and Engineering Management, Editor-in-Chief of the IEEE Transactions on Audio, Speech and Language Processing, Member of the IEEE Signal Processing Society Board of Governors, ISCA Board Member and presently Member of the ISCA International Advisory Council.  She was elected APSIPA's inaugural Distinguished Lecturer 2012-2013 and ISCA Distinguished Lecturer 2015-2016. Her awards include the Ministry of Education Higher Education Outstanding Scientific Research Output Award 2009, Hong Kong Computer Society's inaugural Outstanding ICT Woman Professional Award 2015, Microsoft Research Outstanding Collaborator Award 2016 (1 in 32 worldwide), IEEE ICME 2016 Best Paper Award, IBM Faculty Award 2016, HKPWE Outstanding Women Professionals and Entrepreneurs Award 2017 (1 in 20 since 1999), Hong Kong ICT Silver Award 2018 in Smart Inclusion, and the CogInfoComm2018 Best Paper Award.  Helen received all her degrees from MIT. Her research interests include big data decision analytics, and artificial intelligence especially for speech and language technologies to support multilingual and multimodal human-computer interaction.  Helen has given invited / keynote presentations including INTERSPEECH 2018 Plenary Talk, World Economic Forum Global Future Council 2018, Taihe Workshop on Building Stakeholder Networks on AI Ethics and Governance 2019 and the World Peace Forum 2019. She has served in numerous Government appointments, including Chairlady of the Research Grants Council's Assessment Panel for Competitive Research Funding Schemes for the Local Self-financing Degree Sector, Chairlady of the Working Party on Manpower Survey of the Information/Innovation Technology Sector (since 2013), as well as Steering Committee Member of Hong Kong's Electronic Health Record (eHR) Sharing.  Helen is a Fellow of HKCS, HKIE, IEEE and ISCA.
     </p>
     <br>
-->
     
     </div>
    </section>
    <!-- Speakers Section End -->

    <div id="footerContent"></div>

    <!-- Go to Top Link -->
    <a href="#header-wrap" class="back-to-top">
    <i class="icon-arrow-up"></i>
    </a>

     

    <!-- jQuery Load -->    
    <script src="assets/js/jquery.min.js.txt"></script>
    <!-- Bootstrap JS -->
    <script src="assets/js/bootstrap.min.js.txt"></script> 
    <!-- Countdown Js -->
    <script src="assets/js/jquery.countdown.min.js.txt"></script>
    <!-- Smooth scroll JS -->   
    <script src="assets/js/smooth-scroll.js.txt"></script>        
    <!-- Wow Scroll -->
    <script src="assets/js/wow.js.txt"></script>
    <!-- Owl carousel -->
    <script src="assets/js/owl.carousel.min.js.txt"></script>
    <!-- Slicknav js -->
    <script src="assets/js/jquery.slicknav.js.txt"></script>
    <!--  Nivo lightbox Js -->
    <script src="assets/js/nivo-lightbox.js.txt"></script>   
    <!-- Contact Form Scripts -->
    <script src="assets/js/form-validator.min.js.txt"></script>  
    <script src="assets/js/contact-form-script.js.txt"></script>    
 
    <!-- All Js plugin -->
    <script src="assets/js/main.js.txt"></script> 
    <!-- Map JS -->
    <script type="text/javascript" src="assets/js/jquery.mapit.min.js.txt"></script>
    <script src="assets/js/initializers.js.txt"></script>

  </body>
</html>
